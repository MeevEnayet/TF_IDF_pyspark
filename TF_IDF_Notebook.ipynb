{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing libraries\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import explode, col, lower, regexp_replace, udf, split, countDistinct, log\n",
    "from pyspark.sql.types import StringType\n",
    "from docx import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define functions and UDFs\n",
    "\n",
    "# Function to read .docx files\n",
    "def read_doc_file(file_path):\n",
    "    doc = Document(file_path)\n",
    "    full_text = []\n",
    "    for para in doc.paragraphs:\n",
    "        full_text.append(para.text)\n",
    "    return '\\n'.join(full_text)\n",
    "\n",
    "# UDF for Spark to process text data from .docx files\n",
    "content_udf = udf(read_doc_file, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/01/20 15:44:52 WARN Utils: Your hostname, L-BKASH-MAC-94.local resolves to a loopback address: 127.0.0.1; using 10.21.176.197 instead (on interface en0)\n",
      "25/01/20 15:44:52 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/01/20 15:44:53 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "#Initialize spark session\n",
    "\n",
    "spark = SparkSession.builder.appName(\"TF-IDF Local DOCX Files\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load and Process document data\n",
    "\n",
    "folder_path = 'docs'\n",
    "doc_files = [os.path.join(folder_path, file) for file in os.listdir(folder_path) if file.endswith(\".docx\")]\n",
    "\n",
    "# DataFrame with contents of each document\n",
    "docs_df = spark.createDataFrame([(file,) for file in doc_files], [\"filename\"])\n",
    "docs_df = docs_df.withColumn(\"text\", content_udf(col(\"filename\")))\n",
    "\n",
    "# Clean and tokenize text\n",
    "docs_df = docs_df.withColumn(\"text\", lower(regexp_replace(col(\"text\"), \"[^a-zA-Z\\\\s]\", \"\")))\n",
    "docs_df = docs_df.withColumn(\"words\", explode(split(col(\"text\"), \"\\\\s+\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+------------------+\n",
      "|      words|            filename|            tf_idf|\n",
      "+-----------+--------------------+------------------+\n",
      "|        few|docs/Weather File...|1.0986122886681096|\n",
      "|       some|docs/Report on Bo...|1.0986122886681096|\n",
      "|     online|docs/Report on Bo...|1.0986122886681096|\n",
      "|    experts|docs/Report on Bo...|0.4054651081081644|\n",
      "|    experts|docs/Weather File...|0.4054651081081644|\n",
      "|        not|docs/Report on Bo...|1.0986122886681096|\n",
      "| bookstores|docs/Report on Bo...|1.0986122886681096|\n",
      "|       will|docs/Weather File...|1.0986122886681096|\n",
      "|temperature|docs/Weather File...|1.0986122886681096|\n",
      "|   document|docs/Big Data Cou...| 2.197224577336219|\n",
      "|    records|docs/Weather File...|1.0986122886681096|\n",
      "|        for|docs/Big Data Cou...|0.4054651081081644|\n",
      "|        for|docs/Weather File...|0.4054651081081644|\n",
      "|      words|docs/Big Data Cou...|1.0986122886681096|\n",
      "| possession|docs/Report on Bo...|1.0986122886681096|\n",
      "|   physical|docs/Report on Bo...|1.0986122886681096|\n",
      "|        say|docs/Weather File...|1.0986122886681096|\n",
      "|       days|docs/Weather File...|1.0986122886681096|\n",
      "|     course|docs/Big Data Cou...|1.0986122886681096|\n",
      "|     reason|docs/Report on Bo...|1.0986122886681096|\n",
      "|       past|docs/Weather File...|1.0986122886681096|\n",
      "|         is|docs/Report on Bo...|0.4054651081081644|\n",
      "|         is|docs/Big Data Cou...|0.8109302162163288|\n",
      "|        but|docs/Big Data Cou...|1.0986122886681096|\n",
      "|      cover|docs/Report on Bo...|1.0986122886681096|\n",
      "|     people|docs/Report on Bo...| 2.197224577336219|\n",
      "| completely|docs/Report on Bo...|1.0986122886681096|\n",
      "|       view|docs/Report on Bo...|1.0986122886681096|\n",
      "|         it|docs/Big Data Cou...|1.0986122886681096|\n",
      "|       data|docs/Big Data Cou...| 2.197224577336219|\n",
      "|fascination|docs/Report on Bo...|1.0986122886681096|\n",
      "|       able|docs/Report on Bo...|1.0986122886681096|\n",
      "|       year|docs/Weather File...|1.0986122886681096|\n",
      "|       have|docs/Report on Bo...|1.0986122886681096|\n",
      "|      books|docs/Report on Bo...| 2.197224577336219|\n",
      "|      drive|docs/Report on Bo...|1.0986122886681096|\n",
      "|     things|docs/Report on Bo...|1.0986122886681096|\n",
      "|        the|docs/Report on Bo...|0.8109302162163288|\n",
      "|        the|docs/Weather File...|1.2163953243244932|\n",
      "|     prized|docs/Report on Bo...|1.0986122886681096|\n",
      "|      humid|docs/Weather File...|1.0986122886681096|\n",
      "|  analytics|docs/Big Data Cou...| 2.197224577336219|\n",
      "|       like|docs/Report on Bo...|1.0986122886681096|\n",
      "|        and|docs/Report on Bo...|3.2958368660043287|\n",
      "|       been|docs/Report on Bo...|0.4054651081081644|\n",
      "|       been|docs/Weather File...|0.4054651081081644|\n",
      "|         of|docs/Report on Bo...|1.0986122886681096|\n",
      "|       very|docs/Weather File...|1.0986122886681096|\n",
      "|     stores|docs/Report on Bo...|1.0986122886681096|\n",
      "|        key|docs/Report on Bo...|1.0986122886681096|\n",
      "|        get|docs/Report on Bo...|1.0986122886681096|\n",
      "|       that|docs/Report on Bo...|1.0986122886681096|\n",
      "|      short|docs/Big Data Cou...|1.0986122886681096|\n",
      "|        all|docs/Weather File...|1.0986122886681096|\n",
      "|          a|docs/Report on Bo...|0.4054651081081644|\n",
      "|          a|docs/Big Data Cou...|0.4054651081081644|\n",
      "|    weather|docs/Weather File...|1.0986122886681096|\n",
      "|           |docs/Report on Bo...|0.4054651081081644|\n",
      "|           |docs/Weather File...|0.4054651081081644|\n",
      "|         as|docs/Report on Bo...| 2.197224577336219|\n",
      "|       this|docs/Big Data Cou...|0.8109302162163288|\n",
      "|       this|docs/Weather File...|0.4054651081081644|\n",
      "|  predicted|docs/Report on Bo...|1.0986122886681096|\n",
      "|        has|docs/Big Data Cou...|0.4054651081081644|\n",
      "|        has|docs/Weather File...|0.4054651081081644|\n",
      "|  materials|docs/Report on Bo...|1.0986122886681096|\n",
      "|     exceed|docs/Weather File...|1.0986122886681096|\n",
      "|       test|docs/Big Data Cou...|1.0986122886681096|\n",
      "|    replace|docs/Report on Bo...|1.0986122886681096|\n",
      "|         to|docs/Report on Bo...| 2.197224577336219|\n",
      "|       many|docs/Report on Bo...|1.0986122886681096|\n",
      "|     design|docs/Report on Bo...|1.0986122886681096|\n",
      "|    collect|docs/Report on Bo...|1.0986122886681096|\n",
      "|        big|docs/Big Data Cou...| 2.197224577336219|\n",
      "+-----------+--------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Calculate TF, DF, IDF and TF.IDF\n",
    "\n",
    "# Term Frequency\n",
    "tf = docs_df.groupBy(\"filename\", \"words\").count().alias(\"tf\")\n",
    "\n",
    "# Document Frequency\n",
    "df = tf.groupBy(\"words\").agg(countDistinct(\"filename\").alias(\"df\"))\n",
    "\n",
    "# Inverse Document Frequency\n",
    "total_docs = docs_df.select(\"filename\").distinct().count()\n",
    "idf = df.withColumn(\"idf\", log((total_docs / col(\"df\"))))\n",
    "\n",
    "# Calculate TF-IDF\n",
    "tf_idf = tf.join(idf, \"words\").withColumn(\"tf_idf\", col(\"count\") * col(\"idf\"))\n",
    "\n",
    "# Final DataFrame\n",
    "final_index = tf_idf.select(col(\"words\"), col(\"filename\"), col(\"tf_idf\"))\n",
    "final_index.show(100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stop session\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark_test_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
